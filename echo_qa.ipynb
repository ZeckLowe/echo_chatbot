{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Codes\\Python\\notebooks\\echo_chatbot\\.chatbot\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "import hashlib\n",
    "from pinecone import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "import ast\n",
    "from rapidfuzz import fuzz\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zeck Lowe\\AppData\\Local\\Temp\\ipykernel_34708\\480772498.py:9: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  client = OpenAI(api_key=OPENAI_API_KEY)\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "# Pinecone Initialization\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"echo-openai\")\n",
    "\n",
    "# OpenAI Initialization\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "EMBEDDINGS = OpenAIEmbeddings(model='text-embedding-3-small', openai_api_key=OPENAI_API_KEY)\n",
    "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the QA perspective on the kickoff meeting?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embeddings(query):\n",
    "    \"\"\"\n",
    "    This function returns a list of the embeddings for a given query\n",
    "    \"\"\"\n",
    "    query_embeddings = EMBEDDINGS.embed_query(query)\n",
    "    print(\"Generating Embeddings: Done!\")\n",
    "    return query_embeddings\n",
    "\n",
    "query_embeddings = get_query_embeddings(query=query)\n",
    "print(query_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(title1, title2, threshold=80):\n",
    "    \"\"\"\n",
    "    Perform a fuzzy match between two titles using RapidFuzz.\n",
    "    Returns True if the similarity score is above the threshold.\n",
    "    \"\"\"\n",
    "    similarity_score = fuzz.partial_ratio(title1.lower(), title2.lower())\n",
    "    return similarity_score >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_query(query):\n",
    "  prompt = f\"\"\"\n",
    "  You are a helpful assistant. Extract the meeting title and the meeting date from the following query.\n",
    "  If the meeting title or date is not explicitly mentioned, return 'unknown'.\n",
    "  If the date is mentioned as word, it should be formatted as 'YYYY-MM-DD'\n",
    "\n",
    "  Query: {query}\n",
    "\n",
    "  Provide the meeting title and date as a Python dictionary in this format:\n",
    "  {{\"meeting_title\": \"title_here\", \"date\": \"date_here\"}}\n",
    "  \"\"\"\n",
    "\n",
    "  response = LLM.invoke(prompt)\n",
    "  metadata_str = response.content.strip()\n",
    "  metadata_dict = ast.literal_eval(metadata_str)\n",
    "  return metadata_dict\n",
    "\n",
    "metadata = extract_metadata_from_query(query)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone_index(query_embeddings, meeting_title, date, top_k=2, include_metadata=True):\n",
    "    \"\"\"\n",
    "    Query a Pinecone index.\n",
    "    \"\"\"\n",
    "    filter_conditions = {}\n",
    "    if date.lower() != 'unknown':\n",
    "      filter_conditions['date'] = date\n",
    "    # if meeting_title.lower() != 'unknown':\n",
    "    #   filter_conditions['title'] = meeting_title\n",
    "    # if date.lower() != 'unknown':\n",
    "    #   filter_conditions['date'] = date\n",
    "\n",
    "    query_response = index.query(\n",
    "        vector=query_embeddings,\n",
    "        filter=filter_conditions,\n",
    "        top_k=top_k,\n",
    "        include_metadata=include_metadata,\n",
    "        namespace=\"USJ-R\") # Filter based on metadata\n",
    "    print(query_response)\n",
    "\n",
    "    filtered_matches = []\n",
    "    for match in query_response['matches']:\n",
    "      if 'metadata' in match and 'title' in match['metadata']:\n",
    "        metadata_title = match['metadata']['title']\n",
    "        if fuzzy_match(meeting_title, metadata_title):\n",
    "          filtered_matches.append(match)\n",
    "\n",
    "    if not filtered_matches:\n",
    "      return query_response\n",
    "\n",
    "    query_response['matches'] = filtered_matches\n",
    "\n",
    "    print(\"Querying Pinecone Index: Done!\")\n",
    "    return query_response\n",
    "\n",
    "answers = query_pinecone_index(query_embeddings=query_embeddings, meeting_title=\"kickoff meeting\", date=\"unknown\")\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Text from Multiple Document Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_answer = \" \".join([doc['metadata']['text'] for doc in answers['matches']])\n",
    "print(text_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are a meeting facilitator.\n",
    "        This user will ask you a questions about the conversation of the meeting.\n",
    "        Use following piece of context to answer the question.\n",
    "        If you don't know the answer, just say you don't know.\n",
    "        Keep the answer within 2 sentences and concise.\n",
    "        Context: {text_answers}\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_query_response(prompt):\n",
    "    \"\"\"\n",
    "    This function returns a better response using LLM\n",
    "    \"\"\"\n",
    "    better_answer = LLM.invoke(prompt)\n",
    "    print(\"Generating Better Response: Done!\")\n",
    "    return better_answer\n",
    "\n",
    "final_answer = better_query_response(prompt=prompt)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chatbot(query, meeting_title=None, date=None):\n",
    "    print(query)\n",
    "    metadata = extract_metadata_from_query(query)\n",
    "    print(metadata)\n",
    "    meeting_title = metadata.get('meeting_title', 'unknown')\n",
    "    date = metadata.get('date', 'unknown')\n",
    "\n",
    "    query_embeddings = get_query_embeddings(query=query)\n",
    "\n",
    "    answers = query_pinecone_index(\n",
    "        query_embeddings=query_embeddings,\n",
    "        meeting_title=meeting_title,\n",
    "        date=date\n",
    "        )\n",
    "    print(answers)\n",
    "\n",
    "    text_answers = \" \".join([doc['metadata']['text'] for doc in answers['matches']])\n",
    "    print(text_answers)\n",
    "    final_answer = better_query_response(prompt=prompt)\n",
    "    return final_answer.content\n",
    "\n",
    "response = Chatbot(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
