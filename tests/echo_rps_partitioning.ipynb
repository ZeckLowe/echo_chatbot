{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Codes\\Python\\notebooks\\echo_chatbot\\.chatbot\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import hashlib\n",
    "from pinecone import Pinecone\n",
    "from datetime import date\n",
    "from pinecone import ServerlessSpec\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone Initialization\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "# index = pc.Index(\"echo-openai\")\n",
    "\n",
    "# OpenAI Initialization\n",
    "EMBEDDINGS = OpenAIEmbeddings(model='text-embedding-3-small', openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organization already exists.\n"
     ]
    }
   ],
   "source": [
    "# Pinecone Initalization\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# organization name\n",
    "index_name = input(\"Give organization name\")\n",
    "\n",
    "# If organization name does not exist, it creates new index\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ),\n",
    "        deletion_protection=\"disabled\"\n",
    "    )\n",
    "    # wait for index to be ready\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"Organization already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_size=500):\n",
    "    # Ensure each text ends with a newline to correctly split sentences\n",
    "    if not text.endswith(\"\\n\"):\n",
    "        text += \"\\n\"\n",
    "\n",
    "    # Split text into sentence\n",
    "    sentences = text.split(\"\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # Iterate over sentence and assemble chunks\n",
    "    for sentence in sentences:\n",
    "        # Check if adding the current sentence exceeds the maximum chunk size\n",
    "        if (len(current_chunk) + len(sentences) + 2 > max_chunk_size and current_chunk):\n",
    "            # Add the current chunk to the list and start a new chunk\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \"\"\n",
    "        # Add the current sentence to the current chunk\n",
    "        current_chunk += sentence.strip() + \"\\n\"\n",
    "    # Add any remaining text as the last chunk\n",
    "    if (current_chunk):\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks # type: list[str]\n",
    "\n",
    "# chunked_text = chunk_text(text=text1)\n",
    "# print(chunked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text.\n",
    "    \"\"\"\n",
    "    embedded = EMBEDDINGS.embed_documents(texts)\n",
    "\n",
    "    print(\"Generating embeddings: Done!\")\n",
    "    return embedded\n",
    "\n",
    "# chunked_text_embeddings = generate_embeddings(texts=chunked_text)\n",
    "# print(chunked_text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Vector and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_short_id(content):\n",
    "    \"\"\"\n",
    "    Generate a short ID based on the content using SHA-256 hash.\n",
    "    \"\"\"\n",
    "    hash_obj = hashlib.sha256()\n",
    "    hash_obj.update(content.encode(\"utf-8\"))\n",
    "\n",
    "    print(\"Generating short id: Done!\")\n",
    "    return hash_obj.hexdigest()\n",
    "\n",
    "def combine_vector_and_text(texts, meeting_title, date, text_embeddings):\n",
    "    \"\"\"\n",
    "    Process a list of texts along with their embeddings.\n",
    "    \"\"\"\n",
    "    data_with_metadata = []\n",
    "\n",
    "    for doc_text, embedding in zip(texts, text_embeddings):\n",
    "        if not isinstance(doc_text, str):\n",
    "            doc_text = str(doc_text)\n",
    "\n",
    "        if not isinstance(meeting_title, str):\n",
    "            meeting_title = str(meeting_title)\n",
    "\n",
    "        if not isinstance(date, str):\n",
    "            date = str(date)\n",
    "\n",
    "        text_id = generate_short_id(doc_text)\n",
    "        data_item = {\n",
    "            \"id\": text_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\"text\": doc_text, \"title\": meeting_title, \"date\": date},\n",
    "        }\n",
    "\n",
    "        data_with_metadata.append(data_item)\n",
    "\n",
    "    print(\"Combining vector and text: Done!\")\n",
    "    return data_with_metadata\n",
    "\n",
    "# data_with_meta_data = combine_vector_and_text(texts=chunked_text, meeting_title=meeting_title, date=date, text_embeddings=chunked_text_embeddings)\n",
    "# print(data_with_meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_data_to_pinecone(data_with_metadata, namespace_name):\n",
    "    \"\"\"\n",
    "    Upsert data with metadata into a Pinecone index.\n",
    "    \"\"\"\n",
    "    index.upsert(vectors=data_with_metadata, namespace=namespace_name)\n",
    "    print(\"Upserting vectors to Pinecone: Done!\")\n",
    "\n",
    "# upsert_data_to_pinecone(data_with_metadata=data_with_meta_data, namespace_name=organization)\n",
    "# index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pinecone(texts, meeting_title):\n",
    "    today = str(date.today()) # INITIALIZATION FOR DATE (DYNAMIC) BASED ON STORING\n",
    "    namespace = 'USJ-R' # NAMESPACE DEFAULTED TO 'USJ-R' FOR ISOLATION (STATIC)\n",
    "\n",
    "    chunked_text = chunk_text(text=texts)\n",
    "    chunked_text_embeddings = generate_embeddings(texts=chunked_text)\n",
    "    data_with_meta_data = combine_vector_and_text(texts=chunked_text, meeting_title=meeting_title, date=today,  text_embeddings=chunked_text_embeddings)\n",
    "    upsert_data_to_pinecone(data_with_metadata=data_with_meta_data, namespace_name=namespace)\n",
    "\n",
    "Pinecone(texts=text1, meeting_title=\"Kickoff Meeting for Software Development Project\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
